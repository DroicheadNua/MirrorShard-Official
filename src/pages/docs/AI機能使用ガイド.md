---
layout: ../../layouts/PostLayout.astro
title: AI機能使用ガイド
date: 2026-02-24
---


※本ドキュメントには、開発中の機能についての記述が含まれる場合があります。そのため、配布中のバイナリとは機能が異なる場合があります。  

# MirrorShard 2のAI機能について  
MirrorShard 2 v1.1.0で、これまでの「AIチャットウィンドウ」に加え、エディタ本体から直接AIを呼び出して文章作成やコーディングをサポートする機能が追加されました。  

## メインエディタ統合型AI機能の使い方   

### 1. AI執筆支援（続きを書く）  

小説やドキュメントの執筆中、次に続く文章をAIに提案させる機能です。  

    操作方法: 執筆中、続きを書きたい位置にカーソルを置き、Alt + Enter を押します。  
    
    中断: ESCキーを押すと中断できます。  

    挙動: カーソルより前の文章（最大2500文字程度）をAIが読み取り、その文脈や文体に合わせて続きの文章を自動的に追記します。  

### 2. 選択範囲のAI加工（要約・リライト・翻訳）  

エディタ上のテキストを選択し、右クリックメニューから特定の加工を依頼する機能です。結果は選択範囲の直後に挿入されます。  

    操作方法: テキストを範囲選択し、右クリックメニュー から以下の項目を選択します。  

        AI: 翻訳 (Translate): 日本語なら英語に、英語なら日本語に翻訳します。  

        AI: 要約 (Summarize): 実行時に目標文字数を指定するダイアログが表示されます。長大な文章を簡潔にまとめたい時に便利です。  

        AI: リライト (Rewrite): 文意を変えずに、より分かりやすい表現や洗練された文体に書き換えます。  

    中断: ESCキーを押すと中断できます。  

    特徴:  

        メインの設定（Max Tokens）を無視して長文の出力を許可する設定になっています（最大8192トークン）。  

        元の文章を残したまま直後に追記されるため、AIの回答を比較・検討しながら編集できます。  

### 3. AIコード補完（コードエディタモード限定）  

「コードエディタモード（Ctrl + K）」の時のみ動作する、プログラミング専用の補完機能です。  

    操作方法: コードの記述中、Alt + Enter を押します。  

    挙動: 「FIM (Fill-In-The-Middle)」という技術を用い、カーソルの前後のコードを解析して、その中間に収まるべき最適なコードを推測して挿入します。  

    推奨モデル:  

        qwen2.5-coder:7b (または 1.5b / 3b) などのコーダー特化モデル。  

        ※通常の対話型AIや「思考型（Thinking）」モデル（qwen3-thinkingなど）では、コードの解説を始めてしまうため正常に機能しない場合があります。

    制限事項:  

        プロジェクト全体のファイル構造はスキャンしません。現在開いているファイルのコンテキストのみを参照する軽量仕様です。  



## AIチャットウィンドウの使い方  

　メインエディタのUIボタン（またはCtrl+Shift+A）で、AIと自由に対話できる専用のチャットウィンドウを開きます。執筆中の相談相手として、キャラクターのロールプレイ相手として、あるいは、TRPGのセッションログのように、AIとの対話そのものを楽しむこともできます。  

### 基本的な使い方  

- メッセージの送信: 画面下部の入力エリアにテキストを入力し、「送信」ボタンかCtrl+Enterを押します。  
- UIボタン（括弧内は対応ショートカット）:  
    - ログのクリア（Ctrl+Shift+C）: 現在のチャットログをクリアします。  
    - 保存（Ctrl+S）: 現在のチャットログを「名前を付けて保存」します。  
    - 別名保存: ファイルに名前をつけて保存します。  
    - 読込（Ctrl+O）: 過去に保存したチャットログ（MirrorShard独自形式, LM Studio形式, Gemini形式に対応）を読み込みます。  
    - 最大化（Win・Linux:F11　Mac:Cmd+Ctrl+F）: ウィンドウを最大化/元に戻します。  
    - 閉じる（Ctrl+Shift+A）: ウィンドウを閉じます。  
- 右クリックメニュー: ウィンドウ内を右クリックすると、ログの保存/読込のほか、コピー/ペーストなどの便利な機能にアクセスできます。
    - チャットログを直接メインエディタに送ることも出来ます。送ったデータは新規テキストファイルとして開かれます。  

### 対話のコントロール  

各メッセージの横に表示されるボタンで、対話を細かくコントロールできます。  
- 編集 : ユーザーのメッセージを編集し、そこからAIの応答を再生成させます。  
- 削除 : そのメッセージ以降の、すべての対話履歴を削除します。  
- 再生成 : AIの応答に満足できない場合、同じ文脈で、もう一度別の応答を生成させます。  
- コピー : AIの応答を、クリップボードにコピーします。  

### 名前とアイコンのカスタマイズ  

　ユーザーとAIの表示名は、設定タブで変更することが出来ます。  
　また、ユーザーやAIにアイコンを設定することもできます。アイコンには任意の画像ファイルを設定することができます。  


## AIの導入方法  

### Geminiの場合  

1. Google AI Studioへのログイン: Googleアカウントで、Google AI Studio にアクセスします。  
2. APIキーの取得:  
　・画面左側の「Get API key」をクリックします。  
　・「Create API key in new project」でAPIキーを生成し、その文字列をコピーしておきます。  
　・※APIキーは他人に知られないよう、厳重にご注意ください。  
3. MirrorShardの設定:  
　・設定ウィンドウで「AI設定」タブを選択し、「API Key」欄にコピーしたAPIキーを貼り付けてください。  

### ローカルAIを使用する場合  

#### LM Studioの場合  

1. LM Studioの準備:  
　・LM Studioを起動し、左メニューの「開発者」タブを開きます。  
　・お好きなモデルをロードします。（モデルをお持ちでない場合は、「探索」タブからダウンロードしてください）  
　・左上隅の「Status:Stopped」と書かれているボタンをクリックしてサーバを起動します（「Status: Running」になります）。  
　・その右の「Settings」から「CORSを有効にする」を有効にします。  
　・画面右側のパネルから「Context」タブを選び、システムプロンプトを記入すると、より望む出力結果が得やすくなります。  

２. MirrorShardの設定:  
　・初期設定のままローカルホストで運用する場合には、そのまま（Endpoint URL: http://127.0.0.1:1234/v1/chat/completions のまま）で設定完了です。
　・LANでアクセスする場合にはLM Studio側の設定（Server Settings）で「ローカルネットワークで提供」をオンにし、MirrorShard側の設定ウィンドウで「AI設定」タブを選択しで、「EndpointURL」のIPアドレスをご自身の環境に合わせて書き換えます。  

#### Ollamaの場合  

Ollamaはコマンドラインベースですが、軽量で動作が安定しているローカルLLM実行環境です。  

    Ollamaの準備:  
    ・公式サイト ( https://ollama.com/ ) からOllamaをダウンロードしてインストールします。  
    ・ターミナル（PowerShellやコマンドプロンプト）を開き、使いたいモデルをプル（ダウンロード）します。  
    例: ollama pull gemma2 または ollama pull llama3  
    ・Ollamaは通常、バックグラウンドで自動的に起動しています（タスクトレイにアイコンがあります）。  

    MirrorShardの設定:  
    ・設定ウィンドウの「AI設定」タブを開きます。  
    ・Endpoint URL: http://127.0.0.1:11434/v1/chat/completions と入力します。（ポート番号がLM Studioとは異なります）  
    ・Model Name: ターミナルでプルしたモデル名を入力します（例: gemma2 や llama3:latest）。※ここが間違っていると動きません。  
    ・「適用」を押して保存します。  

これでOllamaとの対話が可能になります。  


## AIの選択  

### AIの切り替え（メインエディタ）  
サイドバーのタブ一覧上部にある「AIセレクター」で、メインエディタが使用するAIを切り替えられます。  
※AIチャットウィンドウの設定とは独立して保持されます。 

### AIの切り替え（AIチャットウィンドウ）  
AIチャットウィンドウの左上のプルダウンメニューから、GeminiとローカルAIのどちらかを選択します。  

### Geminiのモデル選択
　設定ウィンドウの「Model」欄の右にあるプルダウンメニューから、gemini-2.5-proとgemini-2.5-flashのいずれかを選択できます。  
「Model」欄にモデル名を直接入力することで、それ以外のバージョンのGeminiを使うことも出来ます（後述）  

### Geminiの長所と短所  
　長所：導入が比較的容易な上に非常に高性能  
　短所：通信に時間がかかるため生成が遅いこと、使い方によっては課金が必要になること、データが学習に用いられる可能性があること、Googleの厳格なポリシーに違反する使い方は出来ないこと  

### Gemini 3.0 Pro などの最新モデルへの対応について  
設定画面の「Model」欄で「手動入力」を選択、モデル名を直接入力することで、gemini-3-pro-preview などの最新モデルを指定することも可能です。  
【重要：料金について】  
デフォルトの選択肢（Gemini 2.5 Pro等）は無料枠（Free Tier）に対応していますが、カスタム設定で指定可能な Gemini 3系などの最新モデルには無料枠が提供されていない場合があります。  
Google Cloudで課金を有効にしているアカウントでこれらのモデルを使用すると、API利用料が発生する可能性があります。ご利用の際はGoogleの料金プランを十分にご確認ください。  

### ローカルAIの長所と短所  
　長所：ローカルで運用できるためデータが学習されないこと、十分なマシンスペック（特にグラフィックボード）があればGeminiに較べてレスポンスがかなり高速なこと、多様なモデルを使い分けられること、課金が不要なこと、ポリシーが緩いこと  
　短所：導入が比較的複雑なこと、Geminiに較べて性能が劣ること、低スペックなマシンではGeminiより却って遅くなること、LM Studio（またはOllama）を常駐させておかなければならないのでメモリ消費量が増えること  


## AI機能使用上の注意  

### ⚠️ AI機能（Gemini使用時）の利用料金について  

AIチャット機能でGeminiを選択した場合、Google社の Gemini API を使用することになります。  
APIの利用料金は、**ユーザーご自身のGoogleアカウントおよびGoogle Cloudプロジェクトの契約状況**に依存します。  

*   **無料枠（Free Tier）での利用:**  
    *   クレジットカード情報を登録していない、または課金設定を無効にしている場合、無料枠の範囲内で利用可能です。  
    *   無料枠の上限に達した場合、APIはエラーを返し、それ以上の利用はできなくなります（勝手に課金されることはありません）。  
*   **従量課金（Pay-as-you-go）での利用:**  
    *   Google Cloudプロジェクトで課金を有効にしている場合、無料枠を超えた利用分や、無料枠の対象外である高性能モデル（Gemini 3系など）の利用に対して、**Google社から料金が請求される可能性があります。**  

**【免責事項】**  
APIの利用に伴い発生した料金について、本ソフトウェア開発者は一切の責任を負いません。  
ご利用の際は、必ず [Google Gemini APIの料金ページ](https://ai.google.dev/pricing) をご確認の上、ご自身の判断でモデルの選択や利用頻度の調整を行ってください。  

### [Gemini APIキーの取り扱いについて]  
・APIキーは、あなたのアカウントに紐づく「秘密の鍵」です。他人に知られないよう、厳重に管理してください。  
・万が一APIキーが第三者に流出した場合、あなたの無料利用枠が不正に消費されたり、予期せぬ問題が発生したりする可能性があります。  

### [AI切り替え時のコンテクスト（文脈）について]  
　AI使用時、チャットの場合はログの全体がコンテクストとしてAIに送られます。  
　ローカルLLMの中にはセンシティブなコンテンツ（暴力行為、NSFWなど）に対する制約の緩いものもありますが、センシティブな内容のコンテクストをうっかりGeminiに送ってしまったりした場合、Googleの利用規約に抵触して思わぬ事態が発生する場合があります。  
　ご利用の際には、接続先が間違っていないか十分ご確認ください。  

### ログビューアとして使う際の注意点  
　本機能はGoogle AI studioやLM Studioで生成したログを閲覧するビューアとして使うことも出来ますが、読み込めるログの長さには限界があり、数十万トークンあるような長大なログを読み込むとフリーズしてしまいます。  
　こうした長大なログを閲覧する際には、AIチャットウィンドウではなくメインエディタの右クリックメニューから「Geminiログをインポート」を選択し、メインエディタで閲覧されることをお勧めいたします（ただしGemini形式のみ対応）。  
　メインエディタの心臓部であるエディタライブラリ「CodeMirror」は巨大テキストの扱いに強く、数十万トークン程度なら問題なく読み込めます。  

### AIの生成した回答が途中で切れてしまう場合  
　AIの応答長を短く設定した場合などに、ローカルAIの返答が途中で切れてしまうことがあります。  
　これは、AIモデルの指示追従能力が低いために字数内で内容を要約することができず、知っている情報をすべて出力しようとしてしまった結果、MirrorShard側の文字数制限に引っかかって途中で回答が終わってしまうことによって発生します。  
　このようなケースでは、ローカルAI側でシステムプロンプトや応答長の設定をすると改善される場合があります。たとえばLM Studioの場合、「開発者」タブの右側にあるパネルで、「Context」タブや「Inference」タブから各種設定を行ってください。  

### Gemini 2.5 Flash 等で回答が極端に短くなる場合  
　Gemini 2.5 Flash / Pro などのモデルや、一部のローカルLLM（推論強化モデルなど）を使用している際、回答が極端に短くなってしまうことがあります（特にメインエディタのAI執筆機能の使用時）  

#### 原因  
　これらのモデルは、回答を出力する前に内部で**「思考（Thinking）」**を行うプロセスが含まれる場合があります。  
設定画面の「最大応答長」で指定した数値は、この「思考」に使われるトークンも含んだ上限値として扱われます。  
　そのため、設定値が少ない（例: 500～1000程度）と、AIが思考しただけでトークンを使い果たしてしまい、肝心の回答を出力する前に制限に達して強制終了してしまうことがあります。  

#### 対策  
**「最大応答長」の値を大きく設定してください。**  
　Gemini 2.5 Flash等を使用する場合は、**3000 ～ 5000** 程度、あるいはそれ以上の値に設定することを強く推奨します。  
（Geminiの思考プロセスは毎回発生するとは限りませんが、余裕を持った設定にしておくことで防ぐことができます）  

　※ローカルLLMにおいても、思考過程を出力するモデルを使用する場合は同様の現象が起きるため、長めの設定が必要です。  

## 本エディタのAI機能について  
本エディタは「ユーザーが明示的に要求するまでAIには何もさせない」という思想のもと設計されています。  
したがって、ユーザーが送信処理を行った瞬間にのみ、必要最小限のコンテキストを送信する仕様になっています。このため、バックグラウンドで常に執筆内容を監視・送信し続けている商用AIエディタに比べて、どうしても回答の待ち時間が長くなりますが、それは応答速度よりもユーザーの主体的選択とプライバシー、PCのリソース（CPU/メモリ）を優先しているからです。  


